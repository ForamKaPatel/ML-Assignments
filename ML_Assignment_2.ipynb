{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Medical Price Prediction: Linear Regression from Scratch\n",
                "\n",
                "This notebook implements the Linear Regression algorithm from scratch to predict medical charges. We use only **Numpy** and **Pandas** for the model and **Matplotlib** for visualization.\n",
                "\n",
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing\n",
                "\n",
                "Data preprocessing is crucial for the performance of Linear Regression. We perform the following steps:\n",
                "1. **Categorical Encoding**: We convert 'sex' and 'smoker' into binary values (0 and 1). We apply One-Hot Encoding to the 'region' column.\n",
                "2. **Feature Scaling**: Linear Regression using Gradient Descent performs better when features are on the same scale. We standardize the features to have $\\mu=0$ and $\\sigma=1$.\n",
                "3. **Train-Test Split**: We split the data into 80% training and 20% testing sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "df = pd.read_csv('Medical Price Dataset.csv')\n",
                "\n",
                "# Preprocessing categorical variables\n",
                "df['sex'] = df['sex'].map({'female': 0, 'male': 1})\n",
                "df['smoker'] = df['smoker'].map({'no': 0, 'yes': 1})\n",
                "df = pd.get_dummies(df, columns=['region'], drop_first=True)\n",
                "df = df.astype(float)  # Convert all to float\n",
                "\n",
                "# Split into X and y\n",
                "X = df.drop('charges', axis=1).values\n",
                "y = df['charges'].values.reshape(-1, 1)\n",
                "\n",
                "def standardize(X):\n",
                "    mean = np.mean(X, axis=0)\n",
                "    std = np.std(X, axis=0)\n",
                "    std[std == 0] = 1\n",
                "    return (X - mean) / std\n",
                "\n",
                "X_scaled = standardize(X)\n",
                "\n",
                "def train_test_split_custom(X, y, test_size=0.2):\n",
                "    indices = np.arange(X.shape[0])\n",
                "    np.random.shuffle(indices)\n",
                "    split_idx = int(len(X) * (1 - test_size))\n",
                "    return X[indices[:split_idx]], X[indices[split_idx:]], y[indices[:split_idx]], y[indices[split_idx:]]\n",
                "\n",
                "x_train, x_test, y_train, y_test = train_test_split_custom(X_scaled, y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Linear Regression Implementation\n",
                "\n",
                "The `linear_regression` function below uses Gradient Descent to find the optimal weights ($\\theta$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def linear_regression(x_train, y_train, learning_rate=0.1, epochs=1000):\n",
                "    \"\"\"\n",
                "    Trains a linear regression model using gradient descent.\n",
                "    \"\"\"\n",
                "    m, n = x_train.shape\n",
                "    # Add intercept term (bias)\n",
                "    X_b = np.c_[np.ones((m, 1)), x_train]\n",
                "    theta = np.zeros((n + 1, 1))\n",
                "    cost_history = []\n",
                "    \n",
                "    for i in range(epochs):\n",
                "        predictions = X_b.dot(theta)\n",
                "        errors = predictions - y_train\n",
                "        gradients = (1/m) * X_b.T.dot(errors)\n",
                "        theta = theta - learning_rate * gradients\n",
                "        \n",
                "        cost = (1/(2*m)) * np.sum(np.square(errors))\n",
                "        cost_history.append(cost)\n",
                "        \n",
                "    return theta, cost_history\n",
                "\n",
                "# Train the model\n",
                "theta, cost_history = linear_regression(x_train, y_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Results and Visualization\n",
                "\n",
                "We calculate the $R^2$ score and visualize the cost convergence and prediction accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(X, theta):\n",
                "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
                "    return X_b.dot(theta)\n",
                "\n",
                "y_pred = predict(x_test, theta)\n",
                "\n",
                "def get_r2_score(y_true, y_pred):\n",
                "    ss_res = np.sum((y_true - y_pred)**2)\n",
                "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
                "    return 1 - (ss_res / ss_tot)\n",
                "\n",
                "print(f\"Final R2 Score: {get_r2_score(y_test, y_pred):.4f}\")\n",
                "\n",
                "# Cost convergence plot\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.plot(cost_history, color='green')\n",
                "plt.title('Cost Function Convergence')\n",
                "plt.xlabel('Iterations')\n",
                "plt.ylabel('Cost')\n",
                "plt.show()\n",
                "\n",
                "# Actual vs Predicted scatter plot\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.scatter(y_test, y_pred, alpha=0.6)\n",
                "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
                "plt.title('Actual vs Predicted Charges')\n",
                "plt.xlabel('Actual Charges')\n",
                "plt.ylabel('Predicted Charges')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}