{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic Survival Prediction: KNN and Naive Bayes from Scratch\n",
        "\n",
        "This notebook implements the **K-Nearest Neighbors (KNN)** and **Naive Bayes** algorithms from scratch using only NumPy and Pandas. The models are evaluated on the Titanic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preprocessing\n",
        "\n",
        "Preprocessing is crucial for machine learning. In this section, we:\n",
        "- Handle missing values: Missing 'Age' and 'Fare' values are filled with the median. 'Embarked' is filled with the most common value (mode).\n",
        "- Feature Selection: Removed 'PassengerId', 'Name', 'Ticket', and 'Cabin' as they contain high-cardinality or irrelevant information.\n",
        "- Feature Encoding: Converted 'Sex' and 'Embarked' into numerical values.\n",
        "- Feature Scaling: For KNN, features are normalized (mean=0, std=1) because distance calculations are sensitive to the scale of variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Drop irrelevant columns\n",
        "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "    # Fill missing values\n",
        "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "    # Encode categorical variables\n",
        "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "    df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "    return df\n",
        "\n",
        "def normalize(X):\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "    return (X - mean) / std\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "y_test_labels = pd.read_csv('gender_submission.csv')\n",
        "\n",
        "train_processed = preprocess_data(train_df)\n",
        "test_processed = preprocess_data(test_df)\n",
        "\n",
        "x_train = train_processed.drop('Survived', axis=1).values\n",
        "y_train = train_processed['Survived'].values\n",
        "x_test = test_processed.values\n",
        "y_test = y_test_labels['Survived'].values\n",
        "\n",
        "x_train_norm = normalize(x_train)\n",
        "x_test_norm = normalize(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. K-Nearest Neighbors (KNN)\n",
        "\n",
        "KNN is a non-parametric, lazy learning algorithm. It classifies a point based on the labels of its $k$ closest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Compute Euclidean distances\n",
        "        distances = [np.sqrt(np.sum((x - x_tr)**2)) for x_tr in self.X_train]\n",
        "        # Get k nearest samples' labels\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Return the majority vote\n",
        "        return np.bincount(k_nearest_labels).argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Naive Bayes\n",
        "\n",
        "Gaussian Naive Bayes assumes features follow a normal distribution. It uses Bayes' Theorem to calculate the posterior probability for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y == c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0) + 1e-9 # Add small value to avoid division by zero\n",
        "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._get_prediction(x) for x in X])\n",
        "\n",
        "    def _get_prediction(self, x):\n",
        "        posteriors = []\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[idx])\n",
        "            conditional = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posteriors.append(prior + conditional)\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        numerator = np.exp(- (x - mean)**2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run KNN\n",
        "knn = KNN(k=5)\n",
        "knn.fit(x_train_norm, y_train)\n",
        "knn_preds = knn.predict(x_test_norm)\n",
        "knn_acc = np.mean(knn_preds == y_test)\n",
        "\n",
        "# Run Naive Bayes\n",
        "nb = NaiveBayes()\n",
        "nb.fit(x_train, y_train)\n",
        "nb_preds = nb.predict(x_test)\n",
        "nb_acc = np.mean(nb_preds == y_test)\n",
        "\n",
        "print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
        "print(f\"Naive Bayes Accuracy: {nb_acc:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "models = ['KNN', 'Naive Bayes']\n",
        "accuracies = [knn_acc, nb_acc]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, accuracies, color=['blue', 'green'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylim(0, 1.0)\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}